{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "double_digits_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "266KQvZoMxMv",
        "6sfw3LH0Oycm"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPa95uXvcpcn"
      },
      "source": [
        "# MODIFIED FROM: Classifying Handwritten Digits with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JndnmDMp66FL"
      },
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMqWDc_m6rUC",
        "cellView": "both"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdpn8b90u8Tp"
      },
      "source": [
        "![img](https://www.tensorflow.org/images/MNIST.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7HLCm66Cs2p"
      },
      "source": [
        "**Learning Objectives:**\n",
        "  * Train both a linear model and a neural network to classify handwritten digits from the classic [MNIST](http://yann.lecun.com/exdb/mnist/) data set\n",
        "  * Compare the performance of the linear and neural network classification models\n",
        "  * Visualize the weights of a neural-network hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSEh-gNdu8T0"
      },
      "source": [
        "Our goal is to map each input image to the correct numeric digit. We will create a NN with a few hidden layers and a Softmax layer at the top to select the winning class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NMdE1b-7UIH"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's download the data set, import TensorFlow and other utilities, and load the data into a *pandas* `DataFrame`. Note that this data is a sample of the original MNIST training data; we've taken 20000 rows at random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LJ4SD8BWHeh",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Given imports\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random as rd\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 20\n",
        "pd.options.display.float_format = '{:.3f}'.format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLy3pthUS0D2",
        "cellView": "both"
      },
      "source": [
        "#@title HOMEMADE IMPORTS\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from IPython import display\n",
        "from IPython.display import clear_output\n",
        "import seaborn as sbn\n",
        "\n",
        "import time\n",
        "import zipfile\n",
        "import math\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, signal\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, InputLayer, Input\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adagrad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GJ4ODlIfpbo"
      },
      "source": [
        "mnist_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_train_small.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "# Use just the first 10,000 records for training/validation.\n",
        "N = 10000\n",
        "mnist_dataframe = mnist_dataframe.head(N)\n",
        "\n",
        "#mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
        "mnist_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg0-25p2mOi0"
      },
      "source": [
        "Each row represents one labeled example. Column 0 represents the label that a human rater has assigned for one handwritten digit. For example, if Column 0 contains '6', then a human rater interpreted the handwritten character as the digit '6'.  The ten digits 0-9 are each represented, with a unique class label for each possible digit. Thus, this is a multi-class classification problem with 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dghlqJPIu8UM"
      },
      "source": [
        "Columns 1 through 784 contain the feature values, one per pixel for the 28×28=784 pixel values. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent shades of gray. Most of the pixel values are 0; you may want to take a minute to confirm that they aren't all 0.  For example, adjust the following text block to print out the values in column 72."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLNg2VxqhUZ"
      },
      "source": [
        "Now, let's parse out the labels and features and look at a few examples. Note the use of `loc` which allows us to pull out columns based on original location, since we don't have a header row in this data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfFWWvMWDFrR",
        "colab": {}
      },
      "source": [
        "def parse_labels_and_features(dataset):\n",
        "  \"\"\"Extracts labels and features.\n",
        "  \n",
        "  This is a good place to scale or transform the features if needed.\n",
        "  \n",
        "  Args:\n",
        "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
        "      monochrome pixel values on the remaining columns, in row major order.\n",
        "  Returns:\n",
        "    A `tuple` `(labels, features)`:\n",
        "      labels: A Pandas `Series`.\n",
        "      features: A Pandas `DataFrame`.\n",
        "  \"\"\"\n",
        "  labels = dataset[0]\n",
        "\n",
        "  # DataFrame.loc index ranges are inclusive at both ends.\n",
        "  features = dataset.loc[:,1:784]\n",
        "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
        "  features = features / 255\n",
        "\n",
        "  return labels, features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtoZ3LzuuTLd"
      },
      "source": [
        "## Make double digits\n",
        "* make double digit images with labels 0 - 99"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY2KPmO4u2f5"
      },
      "source": [
        "dd_targets, dd_examples = parse_labels_and_features(mnist_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP3ne01odiKI"
      },
      "source": [
        "def getDDs(n):\n",
        "    ## slow?\n",
        "    new_examples = pd.DataFrame(columns=[r for r in range(1568)])\n",
        "    new_targets = []\n",
        "    for i in range(n):\n",
        "        left = rd.choice(dd_examples.index.values)   \n",
        "        right = rd.choice(dd_examples.index.values)\n",
        "\n",
        "        answer = dd_targets[left]*10 + dd_targets[right]\n",
        "        new_targets += [answer]\n",
        "\n",
        "        bkgrnd = np.zeros((28,56))   \n",
        "        bkgrnd[:,0:28] += dd_examples.loc[left].values.reshape((28,28))\n",
        "        bkgrnd[:,28:56] += dd_examples.loc[right].values.reshape((28,28))\n",
        "        if i%200 == 0:\n",
        "            print (\"Image\",i,\"of\",n,\"\\tLabel =\",answer)\n",
        "            display.clear_output(wait=True)\n",
        "        bkgrnd = pd.Series(bkgrnd.reshape(1568))\n",
        "        new_examples = new_examples.append(bkgrnd, ignore_index=True)\n",
        "    return pd.Series(new_targets), new_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P3PGONVdqmm"
      },
      "source": [
        "N = 8000\n",
        "start = time.time()\n",
        "validation_targets, validation_examples = getDDs(N//5)\n",
        "print(\"Made\",N//10,\"new double-digit images to validate on.\")\n",
        "print(\"Time taken: {:.3f} seconds\".format(time.time()-start))\n",
        "start = time.time()\n",
        "training_targets, training_examples = getDDs(4*N//5)\n",
        "print(\"Made\",N,\"new double-digit images to train on.\")\n",
        "print(\"Time taken: {:.3f} seconds\".format(time.time()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFY_-7vZu8UU"
      },
      "source": [
        "#validation_targets, validation_examples = getDDs(2000)    #(N//5)\n",
        "#training_targets, training_examples = getDDs(6000)      #(4*N//5)\n",
        "#training_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt99UsgP96CN"
      },
      "source": [
        "train_6000, valid_2000 = training_examples, validation_examples\n",
        "train_6000[\"target\"] = training_targets\n",
        "valid_2000[\"target\"] = validation_targets\n",
        "train_6000.to_csv(\"doubledigits_train_6000\")\n",
        "valid_2000.to_csv(\"doubledigits_valid_2000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrnAI1v6u8Uh"
      },
      "source": [
        "Show a random example and its corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-euVJVtu8Ui"
      },
      "source": [
        "rand_example = np.random.choice(training_examples.index)\n",
        "_, ax = plt.subplots()\n",
        "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 56),cmap=\"binary_r\")\n",
        "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
        "ax.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKaiLbKaAwxy"
      },
      "source": [
        "N,len(set(training_targets)),len(set(validation_targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScmYX7xdZMXE"
      },
      "source": [
        "## Task 1: Build a Linear Model for MNIST\n",
        "\n",
        "First, let's create a baseline model to compare against. The `LinearClassifier` provides a set of *k* one-vs-all classifiers, one for each of the *k* classes.\n",
        "\n",
        "You'll notice that in addition to reporting accuracy, and plotting Log Loss over time, we also display a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).  The confusion matrix shows which classes were misclassified as other classes. Which digits get confused for each other?\n",
        "\n",
        "Also note that we track the model's error using the `log_loss` function. This should not be confused with the loss function internal to `LinearClassifier` that is used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpoVC4TSdw5Z"
      },
      "source": [
        "def construct_feature_columns():\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  # There are XXX pixels in each image.\n",
        "  return set([tf.feature_column.numeric_column('pixels', shape=28*56)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMmL89yGeTfz"
      },
      "source": [
        "Here, we'll make separate input functions for training and for prediction. We'll nest them in `create_training_input_fn()` and `create_predict_input_fn()`, respectively, so we can invoke these functions to return the corresponding `_input_fn`s to pass to our `.train()` and `.predict()` calls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeS47Bmn5Ms2"
      },
      "source": [
        "def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
        "  \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
        "\n",
        "  Args:\n",
        "    features: The training features.\n",
        "    labels: The training labels.\n",
        "    batch_size: Batch size to use during training.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns batches of training features and labels during\n",
        "    training.\n",
        "  \"\"\"\n",
        "  def _input_fn(num_epochs=None, shuffle=True):\n",
        "    # Input pipelines are reset with each call to .train(). To ensure model\n",
        "    # gets a good sampling of data, even when number of steps is small, we \n",
        "    # shuffle all the data before creating the Dataset object\n",
        "    idx = np.random.permutation(features.index)\n",
        "    raw_features = {\"pixels\":features.reindex(idx)}\n",
        "    raw_targets = np.array(labels[idx])\n",
        "\n",
        "    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(N+1)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zoGWAoohrwS"
      },
      "source": [
        "def create_predict_input_fn(features, labels, batch_size):\n",
        "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
        "\n",
        "  Args:\n",
        "    features: The features to base predictions on.\n",
        "    labels: The labels of the prediction examples.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns features and labels for predictions.\n",
        "  \"\"\"\n",
        "  def _input_fn():\n",
        "    raw_features = {\"pixels\": features.values}\n",
        "    raw_targets = np.array(labels)\n",
        "    \n",
        "    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "        \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6DjSLZMu8Um"
      },
      "source": [
        "def train_linear_classification_model(\n",
        "                                    periods,\n",
        "                                    learning_rate,\n",
        "                                    steps,\n",
        "                                    batch_size,\n",
        "                                    training_examples,\n",
        "                                    training_targets,\n",
        "                                    validation_examples,\n",
        "                                    validation_targets):\n",
        "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, and a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `LinearClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  #periods = 10\n",
        "\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "                            training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "                            validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "                            training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create a LinearClassifier object.\n",
        "  my_optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  #my_optimizer = tf.compat.v1.estimator. clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.LinearClassifier(\n",
        "                    feature_columns=construct_feature_columns(),\n",
        "                    n_classes=100,\n",
        "                    optimizer=my_optimizer,\n",
        "                    #config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "        # Train the model, starting from the prior state.\n",
        "        classifier.train(\n",
        "            input_fn=training_input_fn,\n",
        "            steps=steps_per_period\n",
        "        )\n",
        "    \n",
        "        # Take a break and compute probabilities.\n",
        "        training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "        training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "        training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,100)\n",
        "            \n",
        "        validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "        validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,100)    \n",
        "        \n",
        "        # Compute training and validation errors.\n",
        "        training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "        #training_accuracy = metrics.accuracy_score()\n",
        "        validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "\n",
        "        # Occasionally print the current loss.\n",
        "        print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "        # Add the loss metrics from this period to our list.\n",
        "        training_errors.append(training_log_loss)\n",
        "        validation_errors.append(validation_log_loss)\n",
        "\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItHIUyv2u8Ur"
      },
      "source": [
        "**Spend 5 minutes seeing how well you can do on accuracy with a linear model of this form. For this exercise, limit yourself to experimenting with the hyperparameters for batch size, learning rate and steps.**\n",
        "\n",
        "Stop if you get anything above about 0.9 accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaiIhIQqu8Uv"
      },
      "source": [
        "# To Do: not working!\n",
        "classifier = train_linear_classification_model(\n",
        "                            periods=3,\n",
        "                            learning_rate=0.03,\n",
        "                            steps=500,\n",
        "                            batch_size=100,\n",
        "                            training_examples=training_examples,\n",
        "                            training_targets=training_targets,\n",
        "                            validation_examples=validation_examples,\n",
        "                            validation_targets=validation_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRWcn24DM3qa"
      },
      "source": [
        "Here is a set of parameters that should attain roughly 0.9 accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGlBMrUoM1K_",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# To Do: not working!\n",
        "_ = train_linear_classification_model(\n",
        "                periods = 10,\n",
        "                learning_rate=0.03,\n",
        "                steps=1000,\n",
        "                batch_size=30,\n",
        "                training_examples=training_examples,\n",
        "                training_targets=training_targets,\n",
        "                validation_examples=validation_examples,\n",
        "                validation_targets=validation_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXvrOgtUR-zD"
      },
      "source": [
        "Next, we verify the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scQNpDePSFjt",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "test_targets, test_examples = getDDs(1000)\n",
        "test_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVaWpWKvSHmu"
      },
      "source": [
        "predict_test_input_fn = create_predict_input_fn(test_examples, test_targets, batch_size=100)\n",
        "\n",
        "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
        "  \n",
        "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
        "print(\"Accuracy on test data: %0.2f\" % accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk095OfpPdOx"
      },
      "source": [
        "## Task 2: Replace the Linear Classifier with a Neural Network\n",
        "\n",
        "**Replace the LinearClassifier above with a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) and find a parameter combination that gives 0.95 or better accuracy.**\n",
        "\n",
        "You may wish to experiment with additional regularization methods, such as dropout. These additional regularization methods are documented in the comments for the `DNNClassifier` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XatDGFKEO374"
      },
      "source": [
        "The code below is almost identical to the original `LinearClassifer` training code, with the exception of the NN-specific configuration, such as the hyperparameter for hidden units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdNTx8jkPQUx"
      },
      "source": [
        "def train_nn_classification_model(\n",
        "    periods,\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    hidden_units,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, as well as a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `DNNClassifier` object.\n",
        "  \"\"\"\n",
        "  n_classes = len(set(training_targets))\n",
        "  #periods = 10\n",
        "  # Caution: input pipelines are reset with each call to train. \n",
        "  # If the number of steps is small, your model may never see most of the data.  \n",
        "  # So with multiple `.train` calls like this you may want to control the length \n",
        "  # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
        "  # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
        "  steps_per_period = steps / periods  \n",
        "  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "            training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "            validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "            training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column('pixels', shape=28*56)]\n",
        "\n",
        "  # Create a DNNClassifier object.\n",
        "  my_optimizer = tf.optimizers.Adagrad(learning_rate=learning_rate)\n",
        "  #my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "                    feature_columns=feature_columns,\n",
        "                    n_classes=n_classes,\n",
        "                    hidden_units=hidden_units,\n",
        "                    optimizer=my_optimizer,\n",
        "                    #config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "        # Train the model, starting from the prior state.\n",
        "        classifier.train(\n",
        "            input_fn=training_input_fn,\n",
        "            steps=steps_per_period\n",
        "        )\n",
        "    \n",
        "        # Take a break and compute probabilities.\n",
        "        training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "        training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "        training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id, n_classes)\n",
        "            \n",
        "        validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "        validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id, n_classes)    \n",
        "        \n",
        "        # Compute training and validation errors.\n",
        "        training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "        validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "        # Occasionally print the current loss.\n",
        "        print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "        # Add the loss metrics from this period to our list.\n",
        "        training_errors.append(training_log_loss)\n",
        "        validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfzsTYGPPU8I"
      },
      "source": [
        "classifier = train_nn_classification_model(\n",
        "                        periods=10,\n",
        "                        learning_rate=0.02,\n",
        "                        steps=1000,\n",
        "                        batch_size=50,\n",
        "                        hidden_units=[100, 100],\n",
        "                        training_examples=training_examples,\n",
        "                        training_targets=training_targets,\n",
        "                        validation_examples=validation_examples,\n",
        "                        validation_targets=validation_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evlB5ubzu8VJ"
      },
      "source": [
        "#mnist_test_dataframe = pd.read_csv(\n",
        "#  \"https://download.mlcc.google.com/mledu-datasets/mnist_test.csv\",\n",
        "#  sep=\",\",\n",
        "#  header=None)\n",
        "\n",
        "test_targets, test_examples = getDDs(1000)\n",
        "\n",
        "predict_test_input_fn = create_predict_input_fn(test_examples, test_targets, batch_size=100)\n",
        "\n",
        "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
        "  \n",
        "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
        "print(\"Accuracy on test data: %0.2f\" % accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kIUIceFeDcg"
      },
      "source": [
        "predict_test_input_fn()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_IRTJ8-cjri"
      },
      "source": [
        "##Homemade model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmSt1w14kvce"
      },
      "source": [
        "######################## Friendlier data\n",
        "x_train = validation_examples.values.reshape(validation_examples.shape[0],28,56)\n",
        "x_test = training_examples.values.reshape(training_examples.shape[0],28,56)\n",
        "y_train, y_test = validation_targets, training_targets\n",
        "\n",
        "######################## Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
        "\n",
        "####################### TF Datasets for input\n",
        "train_ds = tf.data.Dataset.from_tensor_slices( (x_train, y_train) )\n",
        "test_ds = tf.data.Dataset.from_tensor_slices( (x_test, y_test) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ7hptQ9cu3r"
      },
      "source": [
        "input_layer = layers.Input(shape=(28,56,1))\n",
        "x = layers.Conv2D(30, 7, activation='relu')(input_layer) # \n",
        "x = layers.Conv2D(20, 5, activation='relu', padding='same')(x) \n",
        "#x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.Conv2D(10, 3, activation='relu', padding='same')(x) \n",
        "#x = layers.MaxPooling2D((2,4))(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(20, activation='relu')(x)\n",
        "x = layers.Dropout(0.05)(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "####################### Build\n",
        "diy_model = Model(input_layer, output_layer)\n",
        "\n",
        "####################### Compile\n",
        "diy_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=Adam(lr=0.005), #0.0075\n",
        "              metrics=['acc'])\n",
        "    \n",
        "####################### Layer-outputs model\n",
        "layer_outputs = [layer.output for layer in diy_model.layers[1:]]\n",
        "diy_output_model = Model(input_layer, layer_outputs)\n",
        "\n",
        "####################### History containers\n",
        "answers, guesses = [],[]\n",
        "diy_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGveooScpAep"
      },
      "source": [
        "#############################\n",
        "diy_history = diy_model.fit(train_ds.shuffle(N).batch(100),\n",
        "                            validation_data=test_ds.batch(20),  \n",
        "                            epochs=5,  \n",
        "                            verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TrSbHhfMc9KG"
      },
      "source": [
        "#@title Define show_layer_output()\n",
        "\n",
        "def show_layer_output(model, output_model, dataset):\n",
        "    \"\"\"\n",
        "    Inputs: model, output_model is layer outputs of model, dataset=a fully loaded tf Dataset of examples and targets\n",
        "    Outputs: true answers, best guesses, the corresponding prediction probabilities for each,\n",
        "             and the image being predicted\n",
        "    \"\"\"\n",
        "    ############################ its own internal version of guessing()\n",
        "    ############################ always returns image\n",
        "    def guessing(n=1,model=model,dataset=dataset):\n",
        "        answers, guesses, pA, pG = [],[],[],[]\n",
        "        for count in range(n):\n",
        "            take1_ds = dataset.shuffle(10001).take(1)\n",
        "            for img, ans in take1_ds:\n",
        "                ans = ans.numpy()\n",
        "                img = img.numpy()                  # eg. 28x56x1\n",
        "            img = img.reshape((1,) + img.shape)    # eg. 1x50x50x1\n",
        "            guess_set = model.predict(img).flatten()\n",
        "            #guess = tf.random.categorical( guess_set, num_samples=1 ).numpy().squeeze()\n",
        "            guess = np.argmax(guess_set)       \n",
        "            answers += [ans]\n",
        "            guesses += [guess]\n",
        "            pG += [guess_set[guess]]\n",
        "            pA += [guess_set[ans]]\n",
        "            print(\"Answer\",ans,\"\\tGuess\",guess, \"\\tp(A)\",round(pA[count],2),\"\\tp(G)\",round(pG[count],2))\n",
        "            if count%10 == 0:\n",
        "                print ('Processing...',count,\"...\")\n",
        "                display.clear_output(wait=True)\n",
        "        return answers, guesses, pA, pG, img\n",
        "    ############################ \n",
        "\n",
        "    answers, guesses, pA, pG, img = guessing()\n",
        "    print(\"Answer:\",answers[0],\"\\tp(Answer):\",round(pA[0],3))\n",
        "    print(\"Guess:\",guesses[0],\"\\tp(Guess):\",round(pG[0],3))\n",
        "    plt.figure(figsize=(2.5,2.5))\n",
        "    plt.imshow(img[0,:,:,0]) \n",
        "\n",
        "    layer_output_maps = output_model.predict( img )\n",
        "    layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "    for layer_name, layer_map in zip(layer_names, layer_output_maps):\n",
        "    ############################ your included / excluded layers here:\n",
        "        print (\"layer:\",layer_name,\"map shape:\",layer_map.shape)\n",
        "        if not \"dense\" in layer_name and not \"dropout\" in layer_name and not \"flatten\" in layer_name:\n",
        "            #if \"flatten\" in layer_name:\n",
        "    ############################ your image shape here:\n",
        "                #layer_map = layer_map.reshape(1,50,50,10)\n",
        "            n_maps = layer_map.shape[-1]  # number of maps\n",
        "            grid_rows = max(1, n_maps//10)\n",
        "            # Map has shape (1, rows, columns, n_maps)\n",
        "            rows = layer_map.shape[1]\n",
        "            cols = layer_map.shape[2]\n",
        "            image_grid = np.zeros((rows * grid_rows, cols * min(10,n_maps)))\n",
        "\n",
        "            for gridrow in range(grid_rows):\n",
        "                for map_n in range(min(10,n_maps)):\n",
        "                        x = layer_map[0, :, :, map_n+10*gridrow]\n",
        "                        #x[-1:1,-1:1] = np.mean(x)\n",
        "                        #x[0:0] = np.max(x)\n",
        "                        #x[0:1,0:5] = np.mean(x)\n",
        "                        #[0:1,5:] = np.max(x)\n",
        "                        #x *= 255.0\n",
        "                        image_grid[gridrow*rows:(gridrow+1)*rows , map_n*cols:(map_n+1)*cols] = x\n",
        "                        \n",
        "            scale = 2.2          \n",
        "            plt.figure(figsize=(scale * 10, scale * grid_rows))\n",
        "            plt.title(layer_name)\n",
        "            plt.grid(False)\n",
        "            plt.imshow(image_grid, cmap='gray')\n",
        "\n",
        "    return answers, guesses, pA, pG, img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_opXHW7VadjH"
      },
      "source": [
        "answers, guesses, pA, pG, img = show_layer_output(diy_model,diy_output_model,test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX2mQBAEcisO"
      },
      "source": [
        "## Task 3: Visualize the weights of the first hidden layer.\n",
        "\n",
        "Let's take a few minutes to dig into our neural network and see what it has learned by accessing the `weights_` attribute of our model.\n",
        "\n",
        "The input layer of our model has `784` weights corresponding to the `28×28` pixel input images. The first hidden layer will have `784×N` weights where `N` is the number of nodes in that layer. We can turn those weights back into `28×28` images by *reshaping* each of the `N` `1×784` arrays of weights into `N` arrays of size `28×28`.\n",
        "\n",
        "Run the following cell to plot the weights. Note that this cell requires that a `DNNClassifier` called \"classifier\" has already been trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngLke_Af81rw"
      },
      "source": [
        "####Original canned-DNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaD97PZ3Z0_q"
      },
      "source": [
        "print(classifier.get_variable_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUC0Z8nbafgG",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "weights0 = classifier.get_variable_value(\"dnn/hiddenlayer_0/kernel\")\n",
        "\n",
        "print(\"weights0 shape:\", weights0.shape)\n",
        "\n",
        "num_nodes = weights0.shape[1]\n",
        "num_rows = int(math.ceil(num_nodes / 10.0))\n",
        "fig, axes = plt.subplots(num_rows, 10, figsize=(40, 2 * num_rows))\n",
        "for coef, ax in zip(weights0.T, axes.ravel()):\n",
        "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
        "    ax.matshow(coef.reshape(28, 56), cmap=plt.cm.viridis)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33INB9GW89z6"
      },
      "source": [
        "####DIY model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qayUoEbRattD"
      },
      "source": [
        "for v in diy_model.variables:\n",
        "    print(v.name, v.shape)\n",
        "#diy_model.get_layer('dense_14').kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUse3K_7dTw"
      },
      "source": [
        "16*44"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "-t5u_D1ptdNW",
        "colab": {}
      },
      "source": [
        "weights = diy_model.get_layer('dense_51').kernel.numpy()\n",
        "weights = weights.reshape(704, 10, 20)\n",
        "weights = weights.reshape(704, 200)\n",
        "\n",
        "print(\"weights shape:\", weights.shape)\n",
        "\n",
        "num_nodes = weights.shape[1]\n",
        "num_rows = int(math.ceil(num_nodes / 10.0))\n",
        "fig, axes = plt.subplots(num_rows, 10, figsize=(40, 2 * num_rows))\n",
        "for coef, ax in zip(weights.T, axes.ravel()):\n",
        "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
        "    ax.matshow(coef.reshape(16, 44), cmap=plt.cm.viridis)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL8MEhNgrx9N"
      },
      "source": [
        "The first hidden layer of the neural network should be modeling some pretty low level features, so visualizing the weights will probably just show some fuzzy blobs or possibly a few parts of digits.  You may also see some neurons that are essentially noise -- these are either unconverged or they are being ignored by higher layers.\n",
        "\n",
        "It can be interesting to stop training at different numbers of iterations and see the effect.\n",
        "\n",
        "**Train the classifier for 10, 100 and respectively 1000 steps. Then run this visualization again.**\n",
        "\n",
        "What differences do you see visually for the different levels of convergence?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Ayu4PwzF9s"
      },
      "source": [
        "##Just NN layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0hUk3iNzM2q"
      },
      "source": [
        "input_layer = layers.Input(shape=(28,56,1))\n",
        "x = layers.Flatten()(input_layer)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "####################### Build\n",
        "nn_model = Model(input_layer, output_layer)\n",
        "\n",
        "####################### Compile\n",
        "nn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=Adam(lr=0.005), #0.0075\n",
        "              metrics=['acc'])\n",
        "    \n",
        "####################### Layer-outputs model\n",
        "layer_outputs = [layer.output for layer in nn_model.layers[1:]]\n",
        "nn_output_model = Model(input_layer, layer_outputs)\n",
        "\n",
        "####################### History containers\n",
        "answers, guesses = [],[]\n",
        "nn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDM0SYuzM2w"
      },
      "source": [
        "#############################\n",
        "nn_history = nn_model.fit(train_ds.shuffle(N).batch(40),\n",
        "                            validation_data=test_ds.batch(40),  \n",
        "                            epochs=5,  \n",
        "                            verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hzMKI_Rzz0I"
      },
      "source": [
        "for v in nn_model.variables:\n",
        "    print(v.name, v.shape)\n",
        "#diy_model.get_layer('dense_14').kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n5ekoZv0q1t"
      },
      "source": [
        "weights = nn_model.get_layer('dense_26').kernel.numpy()\n",
        "print(\"weights shape:\", weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "NIXXPLASzz0P",
        "colab": {}
      },
      "source": [
        "weights = nn_model.get_layer('dense_26').kernel.numpy()\n",
        "#weights = weights.reshape(1568,10,100)\n",
        "#weights = weights.reshape(1568,1000)\n",
        "\n",
        "print(\"weights shape:\", weights.shape)\n",
        "\n",
        "num_nodes = weights.shape[1]\n",
        "num_rows = int(math.ceil(num_nodes / 10.0))\n",
        "fig, axes = plt.subplots(num_rows, 10, figsize=(40, 2 * num_rows))\n",
        "for coef, ax in zip(weights.T, axes.ravel()):\n",
        "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
        "    ax.matshow(coef.reshape(28, 56), cmap=plt.cm.viridis)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}